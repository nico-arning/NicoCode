{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Also zum einen einmal das worüber wir gesprochen haben mit den ATPasen.\n",
    "Dazu gehören die Dateien „Top Features zum Unterscheiden von ATPasen“ (Textdatei der 50\n",
    "wichtigsten Features), „24309_002114-V_plate_1_2h.csv“ und die Plate 2.2h & Plate 3.2h.\n",
    "(Daten aus CellProfiler). Hier hätte ich gern insgesamt drei Plots zum einen für die\n",
    "Behandlung von Concanamycin A (Position auf der Platte sind I1-3 & L7-9), Oligomycin A\n",
    "(Position auf der Platte A1-2 & D7-8) als auch Cyhexatin (Position auf der Platte H1-2 &\n",
    "K7-8). Hierbei habe ich zwar verschieden viele Konzentrationen aber diese liegen alle\n",
    "über dem im CellPainting messenen IC50. Ich würde gerne sehen wie das Ganze aussieht.\n",
    "Wenn es blöd ist wäre ansonsten auch einfach immer die zweite Konzentration super. Also\n",
    "Concanamycin A (I2 & L8), Oligomycin A (A2 & D8) & Cyhexatin (H2 & K8) und natürlich am\n",
    "liebsten über alle drei Platten hinweg. Damit es drei biologische Replikate mit je 2\n",
    "technischen sind.\n",
    "\n",
    "Ich hatte gehofft das wir das Ganze so machen können und die einzelnen Features einfach\n",
    "nummerieren. Und dann in einer Tabelle darunter einfach auflisten.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "- Concanymcen A vs Oligomycin plot DMSO Cyhexatin with it\n",
    "- Ask Bruno\n",
    "- Shap per concentration\n",
    "- Umap / Clustering based on feature select \n",
    "- Baruta\n",
    "- per feature dose response\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treatments = pd.read_excel(\"machine_readable_plate_layout_atpase_2024.xlsx\", index_col=0, sheet_name=\"Treatments\")\n",
    "df_concentrations = pd.read_excel(\"machine_readable_plate_layout_atpase_2024.xlsx\", index_col=0, sheet_name=\"Concentrations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_treatments = df_treatments.reset_index().melt(id_vars='index', var_name='col_index', value_name='Metadata_Treatments')\n",
    "df_treatments.rename(columns={'index': 'row_index'}, inplace=True)\n",
    "\n",
    "df_concentrations = df_concentrations.reset_index().melt(id_vars='index', var_name='col_index', value_name='Metadata_Concentrations')\n",
    "df_concentrations.rename(columns={'index': 'row_index'}, inplace=True)\n",
    "\n",
    "df_treatments['col_index'] = df_treatments['col_index'].astype(str).str.zfill(2)\n",
    "df_concentrations['col_index'] = df_concentrations['col_index'].astype(str).str.zfill(2)\n",
    "\n",
    "df_treatments['Metadata_Well'] = df_treatments['row_index'] + df_treatments['col_index']\n",
    "df_concentrations['Metadata_Well'] = df_concentrations['row_index'] + df_concentrations['col_index']\n",
    "\n",
    "conc_dict = df_concentrations.set_index('Metadata_Well')['Metadata_Concentrations'].to_dict()\n",
    "treat_dict = df_treatments.set_index('Metadata_Well')['Metadata_Treatments'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conc_dict = {k: str(v).strip() for k, v in conc_dict.items()}\n",
    "treat_dict = {k: str(v).strip() for k, v in treat_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_list = [ x.strip() for x in open(\"ATPaseTop50Features.txt\").readlines()]\n",
    "feature_list = list(filter(None, feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"ATPase240309_002114-V_plate_1_2h.csv\", compression=\"gzip\")\n",
    "df_2 = pd.read_csv(\"ATPase240309_012503-V_plate_2_2h.csv\", compression=\"gzip\")\n",
    "df_3 = pd.read_csv(\"ATPase240309_0203012-V_plate3_2h.csv\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_1, df_2, df_3], axis=0, ignore_index=True)\n",
    "df_1, df_2, df_3 = None, None, None # free memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Metadata_Treatments\"] = df[\"Metadata_Well\"].map(treat_dict)\n",
    "df[\"Metadata_Concentrations\"] = df[\"Metadata_Well\"].map(conc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_columns = [col for col in df.columns if col.startswith('Metadata_')]\n",
    "cellranger_feats = [col for col in df.columns if not col.startswith('Metadata_')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape)\n",
    "treatments_of_interest = ['Concanamycin A', 'Oligomycin A', 'Cyhexatin', \"DMSO\"]\n",
    "df = df[df['Metadata_Treatments'].isin(treatments_of_interest)]\n",
    "\n",
    "df['Metadata_Treatments'].replace({\"DMSO\": \"Baseline: DMSO\"}, inplace = True)\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycytominer import feature_select\n",
    "import numpy as np\n",
    "\n",
    "all_ops = [\n",
    "    \"variance_threshold\",\n",
    "    \"correlation_threshold\",\n",
    "    \"drop_na_columns\",\n",
    "    \"blocklist\",\n",
    "]\n",
    "is_this_transcript = False\n",
    "feat_start = np.min(np.where(np.array([k.split('_')[0] for k in cellranger_feats]) =='Image')) if not is_this_transcript else np.where(np.array(col_names) =='Metadata_Plate')[0][0]+1\n",
    "\n",
    "df = feature_select(df, features = cellranger_feats, image_features = False, samples = 'all', operation = all_ops)\n",
    "cellranger_feats = [col for col in df.columns if not col.startswith('Metadata_')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns in feature_list to replace all \"_\" with \" \"\n",
    "df.rename(columns={col: col.replace('_', ' ') for col in cellranger_feats}, inplace=True)\n",
    "cellranger_feats = [ feature.replace(\"_\", \" \") for feature in cellranger_feats ]\n",
    "\n",
    "df.rename(columns={col: col.replace('of4', ' of 4') for col in cellranger_feats}, inplace=True)\n",
    "cellranger_feats = [ feature.replace('of4', ' of 4') for feature in cellranger_feats ]\n",
    "\n",
    "df.rename(columns={col: col.replace('of1', ' of 1') for col in cellranger_feats}, inplace=True)\n",
    "cellranger_feats = [ feature.replace('of1', ' of 1') for feature in cellranger_feats ]\n",
    "\n",
    "df.rename(columns={col: col.replace('of2', ' of 2') for col in cellranger_feats}, inplace=True)\n",
    "cellranger_feats = [ feature.replace('of2', ' of 2') for feature in cellranger_feats ]\n",
    "\n",
    "\n",
    "df.rename(columns={col: col.replace('Only', ' only') for col in cellranger_feats}, inplace=True)\n",
    "cellranger_feats = [ feature.replace('Only', ' only') for feature in cellranger_feats ]\n",
    "# Display the first few rows of the dataframe to verify the changes\n",
    "df.head()\n",
    "cellranger_feats = [ col.replace('Only', ' only') for col in cellranger_feats ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "df[\"Metadata_Treatments\"] = df[\"Metadata_Treatments\"].astype(\"str\")\n",
    "df[\"Metadata_Concentrations\"] = df[\"Metadata_Concentrations\"].astype(\"float\")\n",
    "\n",
    "color_dict = {\n",
    "    'Concanamycin A': '#FF6347',\n",
    "    'Oligomycin A': '#BCEE68',\n",
    "    'Cyhexatin': '#F3D863',\n",
    "    'Baseline: DMSO': '#BEBEBE'\n",
    "}\n",
    "\n",
    "unique_conc = sorted(df[\"Metadata_Concentrations\"].unique())\n",
    "color_palette = sns.color_palette(\"Greys\", len(unique_conc))\n",
    "color_dict_conc = dict(zip(unique_conc, color_palette))\n",
    "\n",
    "# Create a new column for colors\n",
    "df[\"Metadata_colour\"] = df[\"Metadata_Treatments\"].map(color_dict)\n",
    "df[\"Metadata_colour_conc\"] = df[\"Metadata_Concentrations\"].map(color_dict_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_feature_list_by_channel(feature_list):\n",
    "    channels = [\"AGP\", \"ER\", \"Mito\", \"DNA\", \"RNA\", \"mito\"]\n",
    "\n",
    "    feat_dict = {}\n",
    "    for feat in feature_list:\n",
    "        prefix = \"Channel: \"\n",
    "        try:\n",
    "            channel = [channel for channel in channels if channel in feat][0]\n",
    "        except IndexError:\n",
    "            prefix = \"Interchannel Object: \"\n",
    "            channel = feat.split(\" \")[0]\n",
    "        if channel == \"mito\":\n",
    "            channel = \"Mito\" \n",
    "        channel = prefix + channel\n",
    "        if channel not in feat_dict:\n",
    "            feat_dict[channel] = []\n",
    "        feat_dict[channel].append(feat)\n",
    "        \n",
    "    return feat_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_dict = filter_feature_list_by_channel(cellranger_feats)\n",
    "\n",
    "# Get colors from the Set1 palette\n",
    "set1_colors = sns.color_palette(\"Set1\", n_colors=len(feat_dict))\n",
    "\n",
    "# Create a color dictionary\n",
    "color_dict_channels = {key: set1_colors[i] for i, key in enumerate(feat_dict.keys())}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Metadata_Treatments\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.covariance import EllipticEnvelope\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Assuming df is your DataFrame and cellranger_feats is a list of feature names\n",
    "pca = PCA(n_components=2)  # We want to reduce to 2 dimensions for plotting\n",
    "principal_components = pca.fit_transform(df[cellranger_feats])\n",
    "\n",
    "# Create a DataFrame with the principal components\n",
    "pca_df = pd.DataFrame(data=principal_components, columns=[\"PC1\", \"PC2\"])\n",
    "pca_df[\"Metadata_Treatments\"] = list(df[\"Metadata_Treatments\"])\n",
    "pca_df[\"Metadata_Concentrations\"] = list(np.log10(df[\"Metadata_Concentrations\"]))\n",
    "\n",
    "# Fit the Elliptic Envelope model\n",
    "#envelope = EllipticEnvelope(contamination=0.01)  # Adjust contamination as needed\n",
    "#envelope.fit(df[cellranger_feats])\n",
    "# Fit the Elliptic Envelope model\n",
    "envelope = IsolationForest(contamination=\"auto\", n_jobs = -1, random_state = 42)  # Adjust contamination as needed\n",
    "envelope.fit(df[cellranger_feats])\n",
    "\n",
    "pca_df['Outlier'] = envelope.predict(df[cellranger_feats])  # True for outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting\n",
    "plt.figure(figsize=(15, 14))\n",
    "sns.scatterplot(\n",
    "    data=pca_df,\n",
    "    x=\"PC1\", \n",
    "    y=\"PC2\",\n",
    "    hue=\"Metadata_Treatments\",\n",
    "    palette=color_dict,\n",
    "    size=\"Metadata_Concentrations\",\n",
    "    sizes=(20, 200),\n",
    "    alpha=0.7\n",
    ")\n",
    "\n",
    "# Highlight outliers\n",
    "outliers = pca_df[pca_df['Outlier'] == -1]\n",
    "plt.scatter(outliers['PC1'], outliers['PC2'], color=\"black\", edgecolor='black', label='Outliers', s=100, marker='x')\n",
    "\n",
    "plt.title(\"PCA Plot Colored by Metadata Treatments with Outliers Highlighted\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.legend(title=\"Metadata Treatments\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"PCA+outlier.png\")\n",
    "plt.grid()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Metadata_Outliers\"] = list(pca_df['Outlier'])\n",
    "df.loc[df[\"Metadata_Treatments\"] != \"Baseline: DMSO\" , \"Metadata_Outliers\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_df = df[df[\"Metadata_Outliers\"] == -1]\n",
    "print(outlier_df[meta_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df[\"Metadata_Outliers\"] == 1]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "print(df.shape)\n",
    "# Calculate the variance of each feature\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = StandardScaler()\n",
    "df[cellranger_feats] = scaler.fit_transform(df[cellranger_feats])\n",
    "scaler = MinMaxScaler()\n",
    "df[cellranger_feats] = scaler.fit_transform(df[cellranger_feats])\n",
    "# Fit and transform the cellranger columns\n",
    "#variances = df[cellranger_feats].var()\n",
    "#print(variances)\n",
    "# Define a threshold for variance\n",
    "#variance_threshold = 0.01  # You can adjust this threshold as needed\n",
    "\n",
    "# Filter columns based on the variance threshold\n",
    "#high_variance_columns = list(variances[variances > variance_threshold].index)\n",
    "\n",
    "# Filter the dataframe to keep only high variance columns\n",
    "#df = df[meta_columns + high_variance_columns]\n",
    "\n",
    "# Display the first few rows of the filtered dataframe\n",
    "#df.head()\n",
    "#print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for channel, features in feat_dict.items():\n",
    "    print(f\"{channel}: {len(features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert feat_dict\n",
    "inverted_feat_dict = {}\n",
    "for channel, features in feat_dict.items():\n",
    "    for feature in features:\n",
    "        inverted_feat_dict[feature] = channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def perm_importance(X: pd.DataFrame, label_col: str, inverted_feat_dict: dict) -> pd.DataFrame:\n",
    "    \n",
    "    X = X[X[\"Metadata_Concentrations\"].astype(float) >= 1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    X[label_col] = label_encoder.fit_transform(X[label_col])\n",
    "    y = X[label_col]  # Target variable\n",
    "\n",
    "    # Step 2: Prepare the features and target variable\n",
    "    X = X.filter(regex='^(?!Metadata_)')  # Step 1: Encode the categorical column\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Step 3: Train a CatBoost Classifier\n",
    "    model = CatBoostClassifier(random_state=42, verbose=0)  # Set verbose=0 to suppress output during training\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "    y_hat = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_hat)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Step 4: Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model, feature_names=X.columns)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # Step 5: Create a DataFrame for feature importances\n",
    "    # For multi-class classification, we take the mean absolute SHAP values across all classes\n",
    "    if len(shap_values.shape) > 2:\n",
    "        shap_values_mean = pd.DataFrame(np.mean(shap_values, axis=2)).mean()\n",
    "    else:\n",
    "        shap_values_mean = pd.DataFrame(shap_values).mean()\n",
    "\n",
    "    feature_importances = pd.DataFrame({'Feature': X.columns, 'SHAP Feature Importance': shap_values_mean.abs()})\n",
    "    feature_importances[\"Type Feature\"] = [inverted_feat_dict[x] for x in feature_importances[\"Feature\"]]\n",
    "    \n",
    "    # Chi-squared statistics\n",
    "    chi2_stats, p_values = chi2(X, y)\n",
    "    feature_importances[\"Chi2 Statistic\"] = chi2_stats\n",
    "    feature_importances[\"Chi2 p-value\"] = p_values\n",
    "    feature_importances[\"Rank Chi2\"] = feature_importances[\"Chi2 Statistic\"].rank(ascending=False)\n",
    "    feature_importances[\"Rank SHAP\"] = feature_importances[\"SHAP Feature Importance\"].rank(ascending=False)\n",
    "    \n",
    "    feature_importances.loc[feature_importances[\"Chi2 p-value\"] < 0.05, \"Chi2 Statistic\"] = 0\n",
    "\n",
    "    # Logistic Regression for feature importance\n",
    "    log_reg = LogisticRegression(\n",
    "        max_iter=1000,\n",
    "        penalty = None,\n",
    "    )\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    coefficients = log_reg.coef_[0]  # Get the coefficients for the first class\n",
    "    y_hat = log_reg.predict(X_test)\n",
    "    report = classification_report(y_test, y_hat)\n",
    "    print(\"Classification Report Logreg:\")\n",
    "    print(report)\n",
    "    feature_importances[\"Logistic Coefficient\"] = coefficients\n",
    "    feature_importances[\"Rank Logistic\"] = feature_importances[\"Logistic Coefficient\"].rank(ascending=False)\n",
    "\n",
    "    # Sort the features by importance\n",
    "    feature_importances = feature_importances.sort_values(by='Chi2 Statistic', ascending=False)\n",
    "    \n",
    "    # Scale the SHAP feature importances to a 0 to 1 range\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_importances['Scaled SHAP Feature Importance'] = scaler.fit_transform(feature_importances[['SHAP Feature Importance']])\n",
    "    \n",
    "    return feature_importances, shap_values, explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shap\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def perform_anova(feature, X, y):\n",
    "    \"\"\"Function to perform one-way ANOVA for a given feature.\"\"\"\n",
    "    group_data = [X[feature][y == label] for label in np.unique(y)]\n",
    "    f_stat, p_value = stats.f_oneway(*group_data)\n",
    "    return {'Feature': feature, 'F-statistic': f_stat, 'Anova p-value': p_value}\n",
    "\n",
    "def perm_importance(X: pd.DataFrame, label_col: str, inverted_feat_dict: dict) -> pd.DataFrame:\n",
    "    \n",
    "    X = X[X[\"Metadata_Concentrations\"].astype(float) >= 1]\n",
    "    \n",
    "    label_encoder = LabelEncoder()\n",
    "    X[label_col] = label_encoder.fit_transform(X[label_col])\n",
    "    y = X[label_col]  # Target variable\n",
    "\n",
    "    # Step 2: Prepare the features and target variable\n",
    "    X = X.filter(regex='^(?!Metadata_)')  # Step 1: Encode the categorical column\n",
    "\n",
    "    # Split the data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "    # Step 3: Train a CatBoost Classifier\n",
    "    model = CatBoostClassifier(random_state=42, verbose=0)  # Set verbose=0 to suppress output during training\n",
    "    model.fit(X_train, y_train, eval_set=[(X_test, y_test)])\n",
    "    y_hat = model.predict(X_test)\n",
    "    report = classification_report(y_test, y_hat)\n",
    "    print(\"Classification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    # Step 4: Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model, feature_names=X.columns)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # Step 5: Create a DataFrame for feature importances\n",
    "    # For multi-class classification, we take the mean absolute SHAP values across all classes\n",
    "    if len(shap_values.shape) > 2:\n",
    "        shap_values_mean = pd.DataFrame(np.mean(shap_values, axis=2)).mean()\n",
    "    else:\n",
    "        shap_values_mean = pd.DataFrame(shap_values).mean()\n",
    "\n",
    "    feature_importances = pd.DataFrame({'Feature': X.columns, 'SHAP Feature Importance': shap_values_mean.abs()})\n",
    "    feature_importances[\"Type Feature\"] = [inverted_feat_dict[x] for x in feature_importances[\"Feature\"]]\n",
    "    \n",
    "    # Chi-squared statistics\n",
    "    chi2_stats, p_values = chi2(X, y)\n",
    "    feature_importances[\"Chi2 Statistic\"] = chi2_stats\n",
    "    feature_importances[\"Chi2 p-value\"] = p_values\n",
    "    feature_importances[\"Rank Chi2\"] = feature_importances[\"Chi2 Statistic\"].rank(ascending=False)\n",
    "    feature_importances[\"Rank SHAP\"] = feature_importances[\"SHAP Feature Importance\"].rank(ascending=False)\n",
    "\n",
    "    # Logistic Regression for feature importance\n",
    "    log_reg = LogisticRegression(max_iter=1000, penalty=None)\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    coefficients = log_reg.coef_[0]  # Get the coefficients for the first class\n",
    "    y_hat = log_reg.predict(X_test)\n",
    "    report = classification_report(y_test, y_hat)\n",
    "    print(\"Classification Report Logreg:\")\n",
    "    print(report)\n",
    "    feature_importances[\"Logistic Coefficient\"] = coefficients\n",
    "    feature_importances[\"Rank Logistic\"] = feature_importances[\"Logistic Coefficient\"].rank(ascending=False)\n",
    "\n",
    "    # Perform One-Way ANOVA for each continuous feature in parallel\n",
    "    anova_results = Parallel(n_jobs=-1)(  # Use all available cores\n",
    "        delayed(perform_anova)(feature, X, y) for feature in X.columns if np.issubdtype(X[feature].dtype, np.number)\n",
    "    )\n",
    "\n",
    "    anova_df = pd.DataFrame(anova_results)\n",
    "    feature_importances = feature_importances.merge(anova_df, on='Feature', how='left')\n",
    "\n",
    "    # Apply Bonferroni correction\n",
    "    num_tests = len(anova_df)\n",
    "    feature_importances['Bonferroni adjusted Anova p-value'] = feature_importances['Anova p-value'] * num_tests\n",
    "    feature_importances['Bonferroni adjusted Anova p-value'] = feature_importances['Bonferroni adjusted Anova p-value'].clip(upper=1.0)  # Cap at 1.0\n",
    "    \n",
    "    num_tests = len(anova_df)\n",
    "    feature_importances['Bonferroni adjusted Chi2 p-value'] = feature_importances['Chi2 p-value'] * num_tests\n",
    "    feature_importances['Bonferroni adjusted Chi2 p-value'] = feature_importances['Bonferroni adjusted Chi2 p-value'].clip(upper=1.0)  # Cap at 1.0\n",
    "    \n",
    "\n",
    "    # Sort the features by importance\n",
    "    feature_importances = feature_importances.sort_values(by='Chi2 Statistic', ascending=False)\n",
    "    \n",
    "    # Scale the SHAP feature importances to a 0 to 1 range\n",
    "    scaler = MinMaxScaler()\n",
    "    feature_importances['Scaled SHAP Feature Importance'] = scaler.fit_transform(feature_importances[['SHAP Feature Importance']])\n",
    "    \n",
    "    return feature_importances, shap_values, explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances_allVSall, shap_values_allVSall, explainer_all_vs_all = perm_importance(\n",
    "    df[df[\"Metadata_Treatments\"].isin(['Concanamycin A', 'Oligomycin A'])].copy(), \n",
    "    \"Metadata_Treatments\",\n",
    "    inverted_feat_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = []\n",
    "shap_values_list = []\n",
    "explainers = []\n",
    "treatments_feat_imp = ['Concanamycin A', 'Oligomycin A', 'Cyhexatin']\n",
    "control = \"Baseline: DMSO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Metadata_Treatments\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatment in treatments_feat_imp:\n",
    "    print(f\"Processing {treatment}\")\n",
    "    feature_importance, shap_values, explainer = perm_importance(\n",
    "        df[df[\"Metadata_Treatments\"].isin([treatment, control])].copy(),\n",
    "        \"Metadata_Treatments\",\n",
    "        inverted_feat_dict\n",
    "    )\n",
    "    feature_importances.append(feature_importance)\n",
    "    shap_values_list.append(shap_values)\n",
    "    explainers.append(explainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 30\n",
    "feature_importances = [feature_importances_allVSall] + feature_importances\n",
    "shap_values_list = [shap_values_allVSall] + shap_values_list\n",
    "explainers = [explainer_all_vs_all] + explainers\n",
    "treatments_feat_imp = [\"Concanamycin A\"] + treatments_feat_imp\n",
    "controls = [\"Oligomycin A\"] + 3 * [\"DMSO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "df[\"Metadata_Concentrations_log\"] = np.log10(df[\"Metadata_Concentrations\"].astype(float))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import umap\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming df is your DataFrame and feature_importances is defined\n",
    "# Select the top 10 features\n",
    "\n",
    "def top_feat2umap(X, feat_imp, n_features, treatment, control, color_dict):\n",
    "    top_features = list(feat_imp[:n_features][\"Feature\"])\n",
    "    data_to_umap = X[top_features]\n",
    "\n",
    "    # Perform UMAP\n",
    "    umap_model = umap.UMAP()\n",
    "    umap_results = umap_model.fit_transform(data_to_umap)\n",
    "\n",
    "    # Create a new DataFrame for UMAP results\n",
    "    umap_df = pd.DataFrame(umap_results, columns=['UMAP1', 'UMAP2'])\n",
    "    umap_df['Metadata_Treatments'] = df['Metadata_Treatments']\n",
    "    umap_df['Metadata_Concentrations_log'] = df['Metadata_Concentrations_log']\n",
    "\n",
    "    # Plot using seaborn\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(\n",
    "        data=umap_df,\n",
    "        x='UMAP1',\n",
    "        y='UMAP2',\n",
    "        hue='Metadata_Treatments',\n",
    "        palette=color_dict,\n",
    "        size='Metadata_Concentrations_log',\n",
    "        #alpha=0.7\n",
    "    )\n",
    "    plt.title(f\"UMAP of Top {n_features} Features from classification {treatment} vs. {control}\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.savefig(f\"UMAP_{treatment}_vs_{control}_top_{n_features}_feat.png\", dpi = 300)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_feat_imp(df, color_dict, feature_importance, feat_imp, feature, treatment, control, color_dict_channels, added_str=\"\", n: int = 20, p_value_column=None):\n",
    "    \n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(14, int(n / 2)))\n",
    "    feature_importance.sort_values(feat_imp, ascending=False, inplace=True)\n",
    "    \n",
    "    if p_value_column:\n",
    "        feature_importance = feature_importance[feature_importance[p_value_column] <= 0.05]\n",
    "        number_relevant_features = feature_importance.shape[0]\n",
    "        if feature_importance.shape[0] < n:\n",
    "            n = feature_importance.shape[0]\n",
    "            if n <= 1:\n",
    "                print(f\"No significant features with {feat_imp}\")\n",
    "                return\n",
    "\n",
    "    feature_importance = feature_importance[:n].reset_index()\n",
    "    df = df[df[\"Metadata_Concentrations\"] >= 1]\n",
    "    df = df[list(feature_importance[\"Feature\"]) + [\"Metadata_Treatments\"]]\n",
    "\n",
    "    \n",
    "    # Create the bar plot\n",
    "    sns.barplot(\n",
    "        data=feature_importance,\n",
    "        x=feat_imp,\n",
    "        y=feature,\n",
    "        hue=\"Type Feature\",\n",
    "        palette=color_dict_channels,\n",
    "        alpha=0.6,\n",
    "        ax=axes[0]\n",
    "    )\n",
    "    \n",
    "    if p_value_column:\n",
    "        # Annotate bars with p-values\n",
    "        for index, row in feature_importance.iterrows():\n",
    "            axes[0].text(\n",
    "                row[feat_imp] + 0.1,  # Adjust the position slightly to the right of the bar\n",
    "                index,\n",
    "                f' p={row[p_value_column]:.1e}',  # Format p-value to scientific notation\n",
    "                color='black',\n",
    "                va='center'  # Vertically center the text\n",
    "            )\n",
    "        # Add a textbox with the count of columns under the threshold\n",
    "        textbox_str = f'Number of Features under Bonferroni adjusted threshold of 0.05: {number_relevant_features}'\n",
    "        plt.text(0.1, -0.1, textbox_str, ha='center', va='center', transform=axes[0].transAxes, fontsize=12, bbox=dict(facecolor='white', alpha=0.5, edgecolor='none'))\n",
    "            \n",
    "    # Remove the right and top spines\n",
    "    axes[0].spines['right'].set_visible(False)\n",
    "    axes[0].spines['top'].set_visible(False)\n",
    "    \n",
    "    # Melt the DataFrame\n",
    "    long_form_df = pd.melt(\n",
    "        df, \n",
    "        id_vars=['Metadata_Treatments'],  # Columns to keep as identifier variables\n",
    "        var_name='Feature',               # Name for the variable column\n",
    "        value_name='Value Normalised'\n",
    "    )\n",
    "    \n",
    "    # Set the order of the 'Feature' column based on feature_list\n",
    "    long_form_df['Feature ordered'] = pd.Categorical(\n",
    "        long_form_df['Feature'],\n",
    "        categories=list(feature_importance[\"Feature\"]),\n",
    "        ordered=True\n",
    "    )\n",
    "\n",
    "    # Sort the DataFrame by the 'Feature' column\n",
    "    sorted_long_form_df = long_form_df.sort_values(by='Feature ordered')\n",
    "    \n",
    "    # Create the violin plot\n",
    "    sns.stripplot(\n",
    "        data=long_form_df,\n",
    "        x=\"Value Normalised\",\n",
    "        y=\"Feature\",\n",
    "        hue=\"Metadata_Treatments\",\n",
    "        palette=color_dict,\n",
    "        ax=axes[1],\n",
    "        dodge=True,\n",
    "        alpha=0.25,\n",
    "        zorder=1,\n",
    "        legend=False,\n",
    "        jitter=False,\n",
    "    )\n",
    "    \n",
    "    sns.pointplot(\n",
    "        data=long_form_df,\n",
    "        hue='Metadata_Treatments',\n",
    "        x=\"Value Normalised\",\n",
    "        y=\"Feature\",\n",
    "        palette=color_dict,\n",
    "        ax=axes[1],\n",
    "        dodge=.8 - .8 / 3,\n",
    "        linestyle=\"none\",\n",
    "        legend=True\n",
    "    )\n",
    "\n",
    "    # Add horizontal lines between each feature in the bar plot\n",
    "    #for i in range(len(feature_importance)):\n",
    "    #    axes[1].hlines(i-0.5, xmin = 0, xmax = 1, color='gray', linewidth=0.7, alpha = 0.5)\n",
    "\n",
    "    axes[1].grid(False)\n",
    "    axes[1].yaxis.set_visible(False) \n",
    "    axes[1].spines['right'].set_visible(False)\n",
    "    axes[1].spines['top'].set_visible(False)\n",
    "\n",
    "    # Move legends outside the bottom of the plot\n",
    "    axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)  # Adjust ncol for legend layout\n",
    "    axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.1), ncol=2)  # Adjust ncol for legend layout\n",
    "\n",
    "    axes[0].set_title(f\"Top {n} Features by {feat_imp} for {treatment} vs. {control}\")\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping of text\n",
    "    plt.savefig(f\"feature_imp_{treatment}{added_str}.png\", dpi = 300)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 20\n",
    "feat_imps = [\n",
    "    \"SHAP Feature Importance\",\n",
    "    #\"Logistic Coefficient\",\n",
    "    #\"Chi2 Statistic\",\n",
    "    \"F-statistic\"\n",
    "]\n",
    "p_palue_columns = [\n",
    "    None,\n",
    "    #None,\n",
    "    #\"Bonferroni adjusted Chi2 p-value\",\n",
    "    \"Bonferroni adjusted Anova p-value\"\n",
    "]\n",
    "\n",
    "for treatment, feature_importance, control in zip(treatments_feat_imp,  feature_importances, controls):\n",
    "    for type_feat_imp, p_value_column in zip(feat_imps, p_palue_columns):\n",
    "        plot_feat_imp(\n",
    "            df[cellranger_feats + [\"Metadata_Treatments\", \"Metadata_Concentrations\"]].copy(),\n",
    "            color_dict,\n",
    "            feature_importance,\n",
    "            type_feat_imp, \n",
    "            \"Feature\",\n",
    "            treatment, \n",
    "            control,\n",
    "            color_dict_channels,\n",
    "            \"\",\n",
    "            n_features,\n",
    "            p_value_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_imp_of_choice=\"F-statistic\"\n",
    "\n",
    "for feature_importance in feature_importances:\n",
    "    feature_importance.sort_values(feature_imp_of_choice,ascending = False, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for treatment, feature_importance, control in zip(treatments_feat_imp, feature_importances, controls):\n",
    "    top_feat2umap(df.copy(), feature_importance, 50, treatment, control, color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for treatment, feature_importance, control in zip(treatments_feat_imp, feature_importances, controls):\n",
    "#    feature_importance_grouped = feature_importance.groupby('Type Feature').sum().reset_index()\n",
    "#    for type_feat_imp in feat_imps:\n",
    "#        plot_feat_imp(\n",
    "#            df[cellranger_feats + [\"Metadata_Treatments\", \"Metadata_Concentrations\"]].copy(),\n",
    "#            color_dict,\n",
    "#            feature_importance_grouped,\n",
    "#            type_feat_imp,\n",
    "#            \"Type Feature\",\n",
    "#            treatment,\n",
    "#            control,\n",
    "#            color_dict_channels,\n",
    "#            \"grouped_by_channel\",\n",
    "#            feature_importance_grouped.shape[0],\n",
    "#            None)\n",
    "#    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for treatment, feature_importance, control in zip(treatments_feat_imp, feature_importances, controls):\n",
    "#    for channel in feat_dict.keys():\n",
    "#        plot_feat_imp(feature_importance[feature_importance[\"Type Feature\"]==channel], \"Feature\", treatment, control, channel, n_features)\n",
    "#    break    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cellranger_df = df.filter(regex='^(?!Metadata_)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#shap_values = explainer_concanamycin.shap_values(cellranger_df)\n",
    "treatments_feat_imp = ['Concanamycin A vs. DMSO', 'Oligomycin A vs. DMSO', 'Cyhexatin vs. DMSO', \"Concanamycin A vs. Oligomycin A\"]\n",
    "\n",
    "for explainer, treatment in zip(explainers, treatments_feat_imp):\n",
    "    shap_obj = explainer(cellranger_df)\n",
    "    shap.plots.beeswarm(\n",
    "        shap_obj,\n",
    "        max_display=15,\n",
    "        order = shap_obj.abs.max(0),    \n",
    "        show = False\n",
    "    )\n",
    "    plt.title(f\"SHAP Values for {treatment}\")\n",
    "    plt.savefig(f\"SHAP_{treatment}.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # Get current axe\n",
    "    shap.plots.violin(\n",
    "        shap_obj,\n",
    "        max_display=15,\n",
    "        show = False\n",
    "    )\n",
    "    plt.title(f\"SHAP Values for {treatment}\")\n",
    "    plt.savefig(f\"SHAP_violin_{treatment}.png\", dpi = 300)\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 50\n",
    "feature_list = list(feature_importances_allVSall[:n_features][\"Feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "df_group_treat_conc = df.groupby(['Metadata_Treatments', 'Metadata_Concentrations']).mean().reset_index()\n",
    "df_group_treat_conc.index = df_group_treat_conc['Metadata_Treatments'].astype(str) + \" \" + df_group_treat_conc['Metadata_Concentrations'].astype(str)\n",
    "df_group_treat_conc[\"Metadata_colour\"] = df_group_treat_conc[\"Metadata_Treatments\"].astype(str).map(color_dict)\n",
    "df_group_treat_conc[\"Metadata_colour_conc\"] = df_group_treat_conc[\"Metadata_Concentrations\"].map(color_dict_conc)\n",
    "\n",
    "df_group_treat = df.groupby(['Metadata_Treatments']).mean().reset_index()\n",
    "df_group_treat[\"Metadata_colour\"] = df_group_treat[\"Metadata_Treatments\"].astype(str).map(color_dict)\n",
    "df_group_treat[\"Metadata_colour_conc\"] = df_group_treat[\"Metadata_Concentrations\"].map(color_dict_conc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Set the style and create row colors DataFrame\n",
    "sns.set(style=\"whitegrid\")\n",
    "row_colors = pd.DataFrame(\n",
    "    {\n",
    "        \"Treatments\": df_group_treat_conc[\"Metadata_colour\"],\n",
    "        \"Concentrations\": df_group_treat_conc[\"Metadata_colour_conc\"]\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create the clustermap\n",
    "g = sns.clustermap(\n",
    "    df_group_treat_conc[feature_list],\n",
    "    row_cluster=False,\n",
    "    cmap=\"coolwarm\",\n",
    "    method = \"average\",\n",
    "    metric = \"correlation\",\n",
    "    cbar_pos=None,\n",
    "    figsize=(18, 18),\n",
    "    z_score=1,\n",
    "    row_colors=row_colors,\n",
    "    \n",
    ")\n",
    "\n",
    "# Add horizontal lines every 6 y-values\n",
    "for i in range(6, 6*df_group_treat_conc[\"Metadata_Treatments\"].nunique(), 6):\n",
    "    plt.axhline(y=i, color='white', linewidth=1.5)\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.subplots_adjust(right=0.9)  # Adjust the right side as needed\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"clustermap.png\", dpi=300)\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_features = feature_list + ['Metadata_treatments']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = 20\n",
    "feature_list = list(feature_importances_allVSall[:n_features][\"Feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame\n",
    "long_form_df = pd.melt(\n",
    "    df[feature_list + ['Metadata_Treatments', \"Metadata_Concentrations\"]], \n",
    "    id_vars=['Metadata_Treatments', \"Metadata_Concentrations\"],  # Columns to keep as identifier variables\n",
    "    var_name='Feature',               # Name for the variable column\n",
    "    value_name='Value Normalised'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import parallel_coordinates\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Assuming df_group_treat and feature_list are defined\n",
    "\n",
    "#scaler = MinMaxScaler()\n",
    "#df_group_treat_norm = df_group_treat.copy()\n",
    "#df_group_treat_norm[feature_list] = scaler.fit_transform(df_group_treat_norm[feature_list])\n",
    "\n",
    "# Step 1: Calculate the correlation matrix\n",
    "correlation_matrix = df[feature_list].corr()\n",
    "\n",
    "# Step 2: Perform hierarchical clustering\n",
    "linkage_matrix = linkage(pdist(correlation_matrix), method='ward')\n",
    "\n",
    "# Step 3: Create a dendrogram to get the order of features\n",
    "dendro = dendrogram(linkage_matrix, labels=feature_list, no_plot=True)\n",
    "ordered_features = dendro['ivl']  # Get the ordered feature names\n",
    "\n",
    "# Step 4: Create the figure with subplots\n",
    "fig = plt.figure(figsize=(14, 20))\n",
    "\n",
    "# Create the parallel coordinates subplot\n",
    "#ax_parallel = fig.add_subplot(211)  # Parallel coordinates on the top\n",
    "ax = plt.gca()\n",
    "sns.stripplot(\n",
    "    data = long_form_df,\n",
    "    hue = 'Metadata_Treatments',\n",
    "    x = \"Value Normalised\",\n",
    "    y = \"Feature\",\n",
    "    palette = color_dict,\n",
    "    ax = ax,\n",
    "    dodge = True,\n",
    "    alpha = 0.25,\n",
    "    zorder = 1,\n",
    "    legend = False\n",
    ")\n",
    "\n",
    "sns.pointplot(\n",
    "    data = long_form_df,\n",
    "    hue = 'Metadata_Treatments',\n",
    "    x = \"Value Normalised\",\n",
    "    y = \"Feature\",\n",
    "    palette = color_dict,\n",
    "    ax = ax,\n",
    "    dodge=.8 - .8 / 3,\n",
    "    linestyle = \"none\",\n",
    "    legend = True\n",
    ")\n",
    "\n",
    "ax.grid(False)\n",
    "\n",
    "# Improve the legend\n",
    "#sns.move_legend(\n",
    "#    ax, loc=\"lower right\", ncol=3, frameon=True, columnspacing=1, handletextpad=0\n",
    "#)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pointplot_top_n_features.png\", dpi = 300)\n",
    "# ax_parallel.grid(False)\n",
    "\n",
    "# Show the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_no_dmso=df_group_treat_conc[df_group_treat_conc[\"Metadata_Treatments\"] != \"Baseline: DMSO\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the variances in descending order and get the top 10 features\n",
    "n_features = 7\n",
    "top_10_features = list(feature_importances_allVSall[:n_features][\"Feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_newline_at_halfway(sentence):\n",
    "    # Calculate the halfway point\n",
    "    halfway_index = len(sentence) // 2\n",
    "    \n",
    "    # Find the last space before the halfway point to avoid breaking a word\n",
    "    last_space_index = sentence.rfind(' ', 0, halfway_index)\n",
    "    \n",
    "    # If there's no space found, use the halfway index\n",
    "    if last_space_index == -1:\n",
    "        last_space_index = halfway_index\n",
    "    last_space_index +=1\n",
    "    # Insert the newline character\n",
    "    sentence_split = sentence.split()\n",
    "    new_sentence = \" \".join(sentence_split[:2]) + '\\n' + \" \".join(sentence_split[2:]).strip()\n",
    "    \n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "result = feature_importances_allVSall.loc[feature_importances_allVSall.groupby('Type Feature')[\"F-statistic\"].idxmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': False})\n",
    "\n",
    "# Assuming df_no_dmso and top_10_features are defined\n",
    "# Example color_dict, replace with your actual color dictionary\n",
    "\n",
    "pairplot = sns.pairplot(\n",
    "    df[[\"Metadata_Treatments\"] + top_10_features],\n",
    "    kind='scatter',\n",
    "    plot_kws={\n",
    "        \"size\": np.log10(df[\"Metadata_Concentrations\"]),\n",
    "        \"alpha\": 1\n",
    "    },\n",
    "    hue=\"Metadata_Treatments\",\n",
    "    palette=color_dict,\n",
    "    hue_order = sorted(color_dict.keys(), reverse = True),\n",
    "    diag_kind=\"kde\",\n",
    "    height=3,\n",
    "    corner=False,\n",
    "    diag_kws={'fill': True, \"common_norm\": False, \"alpha\": .3, \"linewidth\": 0},  # Fill the KDE plots\n",
    ")\n",
    "\n",
    "# Rotate x and y axis labels\n",
    "for ax in pairplot.axes.flatten():\n",
    "    ax.set_xlabel(ax.get_xlabel(), rotation=90)\n",
    "    ax.set_ylabel(insert_newline_at_halfway(ax.get_ylabel()), rotation=90)\n",
    "\n",
    "# Add regression lines to the upper right corner\n",
    "for treatment in df[\"Metadata_Treatments\"].unique():\n",
    "    for i in range(len(top_10_features)):\n",
    "        for j in range(i + 1, len(top_10_features)):\n",
    "            ax = pairplot.axes[i, j]\n",
    "            for collection in ax.collections:\n",
    "                collection.set_alpha(0.2)  # Set the desired alpha value (0.0 to 1.0)\n",
    "\n",
    "            # Create a regression plot on the current axes\n",
    "            sns.regplot(\n",
    "                x=top_10_features[j], \n",
    "                y=top_10_features[i], \n",
    "                color = color_dict[treatment],\n",
    "                data=df[df[\"Metadata_Treatments\"]==treatment].copy(), \n",
    "                scatter=False, \n",
    "                ax=ax, \n",
    "            )\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Optionally, adjust the subplot parameters for more space\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"pairplot.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "top_10_features = list(result[\"Feature\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "sns.set_style(\"whitegrid\", {'axes.grid': False})\n",
    "\n",
    "# Assuming df_no_dmso and top_10_features are defined\n",
    "# Example color_dict, replace with your actual color dictionary\n",
    "\n",
    "pairplot = sns.pairplot(\n",
    "    df[[\"Metadata_Treatments\"] + top_10_features],\n",
    "    kind='scatter',\n",
    "    plot_kws={\n",
    "        \"size\": np.log10(df[\"Metadata_Concentrations\"]),\n",
    "        \"alpha\": 1\n",
    "    },\n",
    "    hue=\"Metadata_Treatments\",\n",
    "    palette=color_dict,\n",
    "    hue_order = sorted(color_dict.keys(), reverse = True),\n",
    "    diag_kind=\"kde\",\n",
    "    height=3,\n",
    "    corner=False,\n",
    "    diag_kws={'fill': True, \"common_norm\": False, \"alpha\": .3, \"linewidth\": 0},  # Fill the KDE plots\n",
    ")\n",
    "\n",
    "# Rotate x and y axis labels\n",
    "for ax in pairplot.axes.flatten():\n",
    "    ax.set_xlabel(ax.get_xlabel(), rotation=90)\n",
    "    ax.set_ylabel(insert_newline_at_halfway(ax.get_ylabel()), rotation=90)\n",
    "\n",
    "# Add regression lines to the upper right corner\n",
    "for treatment in df[\"Metadata_Treatments\"].unique():\n",
    "    for i in range(len(top_10_features)):\n",
    "        for j in range(i + 1, len(top_10_features)):\n",
    "            ax = pairplot.axes[i, j]\n",
    "            for collection in ax.collections:\n",
    "                collection.set_alpha(0.2)  # Set the desired alpha value (0.0 to 1.0)\n",
    "\n",
    "            # Create a regression plot on the current axes\n",
    "            sns.regplot(\n",
    "                x=top_10_features[j], \n",
    "                y=top_10_features[i], \n",
    "                color = color_dict[treatment],\n",
    "                data=df[df[\"Metadata_Treatments\"]==treatment].copy(), \n",
    "                scatter=False, \n",
    "                ax=ax, \n",
    "            )\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Optionally, adjust the subplot parameters for more space\n",
    "plt.subplots_adjust(left=0.1, right=0.9, top=0.9, bottom=0.1)\n",
    "\n",
    "# Save the plot\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"pairplot_top_per_channel.png\")\n",
    "\n",
    "# Show the plot\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melt the DataFrame\n",
    "long_form_df = pd.melt(\n",
    "    df[feature_list + ['Metadata_Treatments', \"Metadata_Concentrations\"]], \n",
    "    id_vars=['Metadata_Treatments', \"Metadata_Concentrations\"],  # Columns to keep as identifier variables\n",
    "    var_name='Feature',               # Name for the variable column\n",
    "    value_name='Value Normalised'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_form_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "long_form_df[\"Log Concentration\"] = np.log10(long_form_df[\"Metadata_Concentrations\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the variances in descending order and get the top 10 features\n",
    "n_features = 7\n",
    "top_10_features = list(feature_importances_allVSall[:n_features][\"Feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    for i, feature in enumerate(top_features):\n",
    "        fig = plt.figure(figsize = (10,10))\n",
    "        ax = plt.gca()\n",
    "        sns.lineplot(\n",
    "            data = long_form_df[long_form_df[\"Feature\"] == feature].copy(),\n",
    "            y = \"Value Normalised\",\n",
    "            x = \"Log Concentration\",\n",
    "            hue = \"Metadata_Treatments\",\n",
    "            palette = color_dict,\n",
    "            err_style = \"bars\",\n",
    "            ax = ax,\n",
    "            err_kws={'capsize': 6}\n",
    "        )\n",
    "        plt.title(f\"Dose Response Curve for top {i+1}\\n Feature: {feature}\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f\"doseresponce{i+1}_feat_{feature}.png\")\n",
    "        plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bridge_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
